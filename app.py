import os
import streamlit as st
import time
from typing import List, Dict
from datetime import date, timedelta
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from omegaconf import OmegaConf
from retriever import create_retriever
from embedding import embedding_documents
from qdrant_client import QdrantClient
from langchain_upstage import ChatUpstage
from integration_main import Main


load_dotenv()
cfg = OmegaConf.load("config/config.yaml")
filepath = f"./vectorDB/{cfg.split.chunk_type}-{cfg.split.chunk_size}-{cfg.split.chunk_overlap}_{cfg.embedding.backend}-{cfg.embedding.model}-{cfg.embedding.chunk_size}_{cfg.vectordb.backend}"
api_key = os.getenv("UPSTAGE_API_KEY")
#ë¬¸ì„œ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = embedding_documents(cfg.embedding.backend, cfg.embedding.model, cfg.embedding.chunk_size, api_key)
#ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
qdrant_client = QdrantClient(
    host=cfg.vectordb.client.host, 
    port=cfg.vectordb.client.port,
    timeout=100)

#ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ë° ê²€ìƒ‰ ì„¤ì •
retriever = create_retriever(backend=cfg.vectordb.backend, model=embedding_model, filepath=filepath, client=qdrant_client,collection_name=cfg.vectordb.collection_name)
#LLM ëª¨ë¸ ë¡œë“œ
llm = ChatUpstage(model=cfg.llm.model, api_key=api_key, temperature=0)
main = Main(llm,embedding_model,qdrant_client,retriever)
print(main.run(msg="android ai ì— ëŒ€í•´ íƒ€ì„ë¼ì¸ ì‘ì„±í•´ì¤˜",tags=["ai"])) # -> (response,timeline_response) or (response)

# -> tuple -> response[0] : chat_text, response[1] : timeline_datas
# -> str -> response : chat_text


# ========== Page Config ==========
st.set_page_config(page_title="GeekNews Trend Chat", layout="wide")
st.title("ğŸ“° GeekNews Trend Chatbot")

# ========== Sidebar: Controls ==========     ì™¼ìª½ íƒœê·¸, í‚¤ì›Œë“œ ë¶€ë¶„
st.sidebar.header("í•„í„°")
ALL_TAGS = ["ai", "ml", "dl", "cv", "llm", "agent", "infra", "data", "tooling"]     # ì‚¬ìš© ê°€ëŠ¥í•œ íƒœê·¸ ë¦¬ìŠ¤íŠ¸
tags: List[str] = st.sidebar.multiselect("íƒœê·¸ ì„ íƒ", ALL_TAGS, default=["ai","llm"])
keyword: str = st.sidebar.text_input("í‚¤ì›Œë“œ(ì„ íƒ)", placeholder="ì˜ˆ: RAG, LangGraph, MLOps")
period = st.sidebar.selectbox("ê¸°ê°„", ["ìµœê·¼ 7ì¼", "ìµœê·¼ 14ì¼", "ìµœê·¼ 30ì¼", "ì§ì ‘ ì„ íƒ"])
if period == "ì§ì ‘ ì„ íƒ":
    start = st.sidebar.date_input("ì‹œì‘ì¼", value=date.today() - timedelta(days=14))
    end = st.sidebar.date_input("ì¢…ë£Œì¼", value=date.today())
else:
    # 'ìµœê·¼ Nì¼' ì˜µì…˜ì— ë”°ë¼ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    days = int(period.split()[1][:-1])
    start, end = date.today() - timedelta(days=days), date.today()

# 'íƒ€ì„ë¼ì¸ ìƒì„±' ë²„íŠ¼ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ do_timelineì´ Trueê°€ ë©ë‹ˆë‹¤.
do_timeline = st.sidebar.button("íƒ€ì„ë¼ì¸ ìƒì„±")

# ========== Session State ==========
# Streamlitì˜ ì„¸ì…˜ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ë¦¬ë¡œë“œ ê°„ì— ë°ì´í„°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
# 'history'ê°€ ì„¸ì…˜ì— ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•©ë‹ˆë‹¤.
if "history" not in st.session_state:
    st.session_state.history: List[Dict] = []
if "timeline" not in st.session_state:
    st.session_state.timeline: List[Dict] = []
if "main_instance" not in st.session_state:
    # ë°±ì—”ë“œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„¸ì…˜ì— ì €ì¥í•˜ì—¬ ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€
    st.session_state.main_instance = main

# ========== API Key & Client Setup ==========
load_dotenv()

try:
    # LangChainì„ ì‚¬ìš©í•˜ì—¬ Upstage LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    # ChatOpenAI í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©°, Upstageì˜ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ base_urlë¡œ ì§€ì •í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(
        model="solar-1-mini-chat",
        api_key=os.environ["UPSTAGE_API_KEY"],
        base_url="https://api.upstage.ai/v1",
    )
# API í‚¤ê°€ .env íŒŒì¼ì— ì—†ì„ ê²½ìš° ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ê³  ì•± ì‹¤í–‰ì„ ì¤‘ì§€í•©ë‹ˆë‹¤.
except (KeyError, TypeError):
    st.error("âš ï¸ Upstage API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.error("`.env` íŒŒì¼ì— `UPSTAGE_API_KEY = 'YOUR_API_KEY'` í˜•ì‹ìœ¼ë¡œ í‚¤ë¥¼ ì €ì¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# ========== Mock Backend (êµì²´ í¬ì¸íŠ¸) ==========    ì˜¤ë¥¸ìª½ íƒ€ì„ë¼ì¸ ë¶€ë¶„  ***** ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€í•´ì•¼í•¨ *****
def fetch_timeline(tags: List[str], keyword: str, start: date, end: date) -> List[Dict]:
    """
    ê°€ìƒ íƒ€ì„ë¼ì¸ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    TODO: ì‹¤ì œ ë°±ì—”ë“œ APIì™€ ì—°ë™í•˜ì—¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    # ë°±ì—”ë“œ API í˜¸ì¶œ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì§€ì—°ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
    try:
        # ì„¸ì…˜ì— ì €ì¥ëœ main ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
        main_instance = st.session_state.main_instance
        
        # Main.run()ì„ ì‚¬ìš©í•˜ì—¬ íƒ€ì„ë¼ì¸ ìƒì„±
        combined_query = f"íƒ€ì„ë¼ì¸ ìƒì„±"
        if keyword:
            combined_query += f" (í‚¤ì›Œë“œ: {keyword})"
        
        # datetime import ì¶”ê°€
        from datetime import datetime
        
        response = main_instance.run(
            msg=combined_query, 
            tags=tags,
            start_dt=datetime.combine(start, datetime.min.time()),
            end_dt=datetime.combine(end, datetime.max.time()),
            n_buckets=10
        )
        
        if isinstance(response, tuple):
            # íƒ€ì„ë¼ì¸ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
            _, timeline_data = response
            # íƒ€ì„ë¼ì¸ ë°ì´í„°ë¥¼ UIì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_timeline = []
            for item in timeline_data:
                payload = item.get("payload", {})
                # ellipsisê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ contentì—ì„œ ì²« 100ì ì¶”ì¶œ
                content = payload.get("ellipsis", payload.get("content", ""))
                summary = content[:100] + "..." if len(content) > 100 else content
                
                formatted_timeline.append({
                    "date": payload.get("date_str", payload.get("date", "ë‚ ì§œ ì—†ìŒ")),
                    "title": payload.get("title", "ì œëª© ì—†ìŒ"),
                    "summary": summary,
                    "url": payload.get("href", payload.get("source_link", "#")),
                    "tags": tags[:3] or ["ai"]
                })
            return formatted_timeline
        else:
            # íƒ€ì„ë¼ì¸ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
            return []
            
    except Exception as e:
        st.error(f"íƒ€ì„ë¼ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë°ì´í„° ë°˜í™˜
        return [{
            "date": str(end - timedelta(days=i*2)),
            "title": f"[{tags[0] if tags else 'trend'}] {keyword or 'hot topic'} ì—…ë°ì´íŠ¸ {i}",
            "summary": "íƒ€ì„ë¼ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "url": "#",
            "tags": tags[:3] or ["ai"]
        } for i in range(1,3)]

# ========== LangChainì„ ì‚¬ìš©í•˜ì—¬ Upstage APIë¥¼ í˜¸ì¶œí•˜ê³  ì±—ë´‡ ì‘ë‹µì„ ìƒì„± ==========
def chat_api(message: str, tags: List[str], keyword: str) -> dict:   
    # 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ëŠ” í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤.
    # 'tags', 'keyword', 'question' ë³€ìˆ˜ë¥¼ ë°›ì•„ ë™ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful AI assistant specializing in summarizing and explaining the latest tech trends from sources like GeekNews.
Your answer must be in Korean.
The user is currently interested in the following context:
- Tags: {tags}
- Keyword: {keyword}
Based on this context, please provide a concise and clear answer to the user's question."""),
        ("user", "{question}")
    ])

    # 2. LangChain ì²´ì¸ êµ¬ì„± (LCEL)
    # í”„ë¡¬í”„íŠ¸, LLM, ì¶œë ¥ íŒŒì„œë¥¼ íŒŒì´í”„(|)ë¡œ ì—°ê²°í•˜ì—¬ ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.
    # StrOutputParserëŠ” LLMì˜ ì‘ë‹µ(AIMessage)ì—ì„œ ë¬¸ìì—´ ë‚´ìš©ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    chain = prompt_template | llm | StrOutputParser()

    try:
        # 3. ì²´ì¸ ì‹¤í–‰
        # ì²´ì¸ì— í•„ìš”í•œ ë³€ìˆ˜ë“¤ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬í•˜ì—¬ ì‹¤í–‰(invoke)í•©ë‹ˆë‹¤.
        answer = chain.invoke({
            "tags": tags or 'Not specified',
            "keyword": keyword or 'Not specified',
            "question": message
        })
        main_instance = st.session_state.main_instance
            
            # integration_main.pyì˜ Main í´ë˜ìŠ¤ ì‚¬ìš©
            # ë©”ì‹œì§€ì™€ íƒœê·¸ë¥¼ ê²°í•©í•˜ì—¬ ì§ˆë¬¸ êµ¬ì„±
        combined_query = f"{message}"
        if keyword:
            combined_query += f" (í‚¤ì›Œë“œ: {keyword})"
            
        # Main.run() í˜¸ì¶œí•˜ì—¬ ì‹¤ì œ LLM ì‘ë‹µ ìƒì„±
        response = main_instance.run(msg=combined_query, tags=tags)
        # UIì™€ í˜¸í™˜ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        return {"answer": answer, "citations": []}
    
        

    # API í˜¸ì¶œ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ Streamlit í™”ë©´ì— ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    except Exception as e:
        st.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "citations": []}
    # 1. ì§ˆë¬¸ íƒ€ì… ë¶„ë¥˜
    # def is_general_conversation(msg: str) -> bool:
    #     """ì¼ë°˜ ëŒ€í™”ì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜"""
    #     general_patterns = [
    #         "ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•í•˜ì…¨ë‚˜ìš”", "ë°˜ê°‘", "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ì£„ì†¡",
    #         "ë‚ ì”¨", "ê¸°ë¶„", "ì»¤í”¼", "ìŒì‹", "ì˜í™”", "ìŒì•…", "ì·¨ë¯¸", "ìš´ë™",
    #         "ë„ì›€", "ë­í•´", "ì–´ë–»ê²Œ", "ë¬´ì—‡", "ëˆ„êµ¬", "ì–¸ì œ", "ì–´ë””", "ì™œ",
    #         "ì¢‹ì•„", "ì‹«ì–´", "ì¬ë¯¸", "í˜ë“¤", "ì‰¬ì›Œ", "ì–´ë ¤ì›Œ", "ë§ì•„", "í‹€ë ¤",
    #         "ê·¸ë˜", "ì•„ë‹ˆ", "ë§ì•„", "í‹€ë ¤", "ì•Œê² ", "ëª¨ë¥´", "ê¶ê¸ˆ", "ê¶ê¸ˆí•´"
    #     ]
        
    #     msg_lower = msg.lower()
    #     return any(pattern in msg_lower for pattern in general_patterns)
    
    # def is_search_query(msg: str) -> bool:
    #     """ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜"""
    #     search_patterns = [
    #         "íŠ¸ë Œë“œ", "ë™í–¥", "ìµœì‹ ", "ìµœê·¼", "í˜„ì¬", "ë°œì „", "ë³€í™”", "ì—…ë°ì´íŠ¸",
    #         "ê¸°ìˆ ", "AI", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "LLM", "RAG", "LangChain",
    #         "ë‰´ìŠ¤", "ê¸°ì‚¬", "ì •ë³´", "ë°ì´í„°", "ë¶„ì„", "ì—°êµ¬", "ë…¼ë¬¸", "ë³´ê³ ì„œ",
    #         "ì‹œì¥", "ì‚°ì—…", "íšŒì‚¬", "ì œí’ˆ", "ì„œë¹„ìŠ¤", "í”Œë«í¼", "ë„êµ¬", "ë¼ì´ë¸ŒëŸ¬ë¦¬",
    #         "íƒ€ì„ë¼ì¸", "ì¼ì •", "ê³„íš", "ë¡œë“œë§µ", "ë¯¸ë˜", "ì „ë§", "ì˜ˆì¸¡"
    #     ]
        
    #     msg_lower = msg.lower()
    #     return any(pattern in msg_lower for pattern in search_patterns)
    
    # 2. ì§ˆë¬¸ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬ ë¶„ê¸°
#     if is_general_conversation(message):
#         # ì¼ë°˜ ëŒ€í™”: ì§ì ‘ LLMì— ì§ˆë¬¸
#         try:
#             prompt_template = ChatPromptTemplate.from_messages([
#                 ("system", """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
# ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ê³ , í•„ìš”ì— ë”°ë¼ ë„ì›€ì„ ì œê³µí•˜ì„¸ìš”.
# ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”."""),
#                 ("user", "{question}")
#             ])
            
#             chain = prompt_template | llm | StrOutputParser()
#             answer = chain.invoke({"question": message})
#             return {"answer": answer, "citations": []}
            
#         except Exception as e:
#             st.error(f"ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#             return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "citations": []}
    
#     elif is_search_query(message):
#         # ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸: ë°±ì—”ë“œ RAG ì‹œìŠ¤í…œ ì‚¬ìš©
#         try:
#             # ì„¸ì…˜ì— ì €ì¥ëœ main ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
#             main_instance = st.session_state.main_instance
            
#             # integration_main.pyì˜ Main í´ë˜ìŠ¤ ì‚¬ìš©
#             # ë©”ì‹œì§€ì™€ íƒœê·¸ë¥¼ ê²°í•©í•˜ì—¬ ì§ˆë¬¸ êµ¬ì„±
#             combined_query = f"{message}"
#             if keyword:
#                 combined_query += f" (í‚¤ì›Œë“œ: {keyword})"
            
#             # Main.run() í˜¸ì¶œí•˜ì—¬ ì‹¤ì œ LLM ì‘ë‹µ ìƒì„±
#             response = main_instance.run(msg=combined_query, tags=tags)
            
#             # ì‘ë‹µ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
#             if isinstance(response, tuple):
#                 # íƒ€ì„ë¼ì¸ ì‘ë‹µì¸ ê²½ìš°
#                 chat_text, timeline_data = response
#                 return {
#                     "answer": chat_text, 
#                     "citations": [],
#                     "timeline_data": timeline_data
#                 }
#             else:
#                 # ì¼ë°˜ ì±„íŒ… ì‘ë‹µì¸ ê²½ìš°
#                 return {"answer": response, "citations": []}

#         except Exception as e:
#             st.error(f"ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#             return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "citations": []}
    
#     else:
#         # ê¸°ë³¸ê°’: ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬
#         try:
#             prompt_template = ChatPromptTemplate.from_messages([
#                 ("system", """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
# ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ê³ , í•„ìš”ì— ë”°ë¼ ë„ì›€ì„ ì œê³µí•˜ì„¸ìš”.
# ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”."""),
#                 ("user", "{question}")
#             ])
            
#             chain = prompt_template | llm | StrOutputParser()
#             answer = chain.invoke({"question": message})
#             return {"answer": answer, "citations": []}
            
#         except Exception as e:
#             st.error(f"ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#             return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "citations": []}


# ì‘ë‹µì˜ ê·¼ê±°(Citations)ë¥¼ í™”ë©´ì— ë Œë”ë§í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def render_citations(citations: List[Dict]):
    # ê·¼ê±° ë°ì´í„°ê°€ ìˆì„ ê²½ìš°ì—ë§Œ í™•ì¥ ê°€ëŠ¥í•œ(expander) UIë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    if citations:
        with st.expander("ğŸ” ê·¼ê±° ë³´ê¸°", expanded=False):
            for c in citations:
                st.write(f"- {c['text']}  \n  `{c['source']}`")

# ========== Main Layout ==========
# ë©”ì¸ í™”ë©´ì„ 2ê°œì˜ ì—´(ì±—ë´‡, íƒ€ì„ë¼ì¸)ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.  2:1 ë¹„ìœ¨
main_col, right_col = st.columns([2, 1])

# ì™¼ìª½ ì—´ (ì±—ë´‡ ì˜ì—­)
with main_col:
    st.subheader("ğŸ’¬ ì±—ë´‡")
    # ë†’ì´ê°€ ê³ ì •ëœ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´, ë‚´ìš©ì´ ë„˜ì¹  ê²½ìš° ìŠ¤í¬ë¡¤ì´ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    chat_container = st.container(height=450)
    with chat_container:
        # ì„¸ì…˜ì— ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ ìˆœíšŒí•˜ë©° í™”ë©´ì— ë Œë”ë§í•©ë‹ˆë‹¤.
        for m in st.session_state.history:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])
                render_citations(m.get("citations", []))

    # ì±„íŒ… ì…ë ¥ì°½ ìœ„ì ¯ì„ ìƒì„±í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ë©´ user_msg ë³€ìˆ˜ì— ì €ì¥ë©ë‹ˆë‹¤.
    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”â€¦ (ì˜ˆ: LLM íŠ¸ë Œë“œ í•µì‹¬ ìš”ì•½)")
    if user_msg:
        # 1. ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        st.session_state.history.append({"role": "user", "content": user_msg})
        
        # 2. ì±„íŒ… ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì— ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
        with chat_container:
            with st.chat_message("user"):
                st.markdown(user_msg)

            # 3. ì±—ë´‡ì˜ ë‹µë³€ì„ ìƒì„±í•˜ê³  ë Œë”ë§í•©ë‹ˆë‹¤.
            with st.chat_message("assistant"):
                # 'ë‹µë³€ ìƒì„± ì¤‘' ìŠ¤í”¼ë„ˆë¥¼ í‘œì‹œí•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ëŒ€ê¸° ìƒíƒœì„ì„ ì•Œë¦½ë‹ˆë‹¤.
                with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    resp = chat_api(user_msg, tags, keyword)
                # APIë¡œë¶€í„° ë°›ì€ ë‹µë³€ì„ ë Œë”ë§í•©ë‹ˆë‹¤.
                st.markdown(resp["answer"])
                render_citations(resp.get("citations", []))
            
            # 4. ì±—ë´‡ì˜ ë‹µë³€ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            st.session_state.history.append(
                {"role": "assistant", "content": resp["answer"], "citations": resp.get("citations", [])}
            )

# ì˜¤ë¥¸ìª½ ì—´ (íƒ€ì„ë¼ì¸ ì˜ì—­)
with right_col:
    st.subheader("ğŸ—“ï¸ íƒ€ì„ë¼ì¸")
    # 'íƒ€ì„ë¼ì¸ ìƒì„±' ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œë§Œ íƒ€ì„ë¼ì¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    if do_timeline:
        with st.spinner("íƒ€ì„ë¼ì¸ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            st.session_state.timeline = fetch_timeline(tags, keyword, start, end)
    # ë†’ì´ê°€ ê³ ì •ëœ ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ì–´, íƒ€ì„ë¼ì¸ì´ ê¸¸ì–´ì ¸ë„ ë…ë¦½ì ìœ¼ë¡œ ìŠ¤í¬ë¡¤ë˜ë„ë¡ í•©ë‹ˆë‹¤.
    timeline_container = st.container(height=500)
    with timeline_container:
        # ì„¸ì…˜ì— íƒ€ì„ë¼ì¸ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í™”ë©´ì— ë Œë”ë§í•©ë‹ˆë‹¤.
        if st.session_state.timeline:
            for item in st.session_state.timeline:
                with st.container(border=True): # ê° ì•„ì´í…œì„ í…Œë‘ë¦¬ê°€ ìˆëŠ” ì»¨í…Œì´ë„ˆë¡œ ë¬¶ìŠµë‹ˆë‹¤.
                    st.markdown(f"**{item['title']}**  \n{item['date']} â€¢ {' / '.join(item['tags'])}")
                    st.write(item["summary"])
                    st.markdown(f"[ì›ë¬¸ ë§í¬]({item['url']})")
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
        else:
            st.info("ì¢Œì¸¡ í•„í„°ì—ì„œ **íƒ€ì„ë¼ì¸ ìƒì„±**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
